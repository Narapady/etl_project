[2022-08-11 05:57:46,468] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: ETL_DAG_V2.load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [queued]>
[2022-08-11 05:57:46,486] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: ETL_DAG_V2.load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [queued]>
[2022-08-11 05:57:46,488] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-08-11 05:57:46,489] {taskinstance.py:1377} INFO - Starting attempt 1 of 6
[2022-08-11 05:57:46,490] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-08-11 05:57:46,518] {taskinstance.py:1397} INFO - Executing <Task(S3ToSnowflakeOperator): load_into_food_consumption_estimates_table> on 2022-08-11 05:57:31.582348+00:00
[2022-08-11 05:57:46,540] {standard_task_runner.py:52} INFO - Started process 18179 to run task
[2022-08-11 05:57:46,563] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'ETL_DAG_V2', 'load_into_food_consumption_estimates_table', 'manual__2022-08-11T05:57:31.582348+00:00', '--job-id', '224', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpalg6wh9w', '--error-file', '/tmp/tmpdi3600_s']
[2022-08-11 05:57:46,566] {standard_task_runner.py:80} INFO - Job 224: Subtask load_into_food_consumption_estimates_table
[2022-08-11 05:57:46,728] {task_command.py:371} INFO - Running <TaskInstance: ETL_DAG_V2.load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [running]> on host c9fcec21ee59
[2022-08-11 05:57:46,855] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=narapady
AIRFLOW_CTX_DAG_ID=ETL_DAG_V2
AIRFLOW_CTX_TASK_ID=load_into_food_consumption_estimates_table
AIRFLOW_CTX_EXECUTION_DATE=2022-08-11T05:57:31.582348+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-08-11T05:57:31.582348+00:00
[2022-08-11 05:57:46,862] {s3_to_snowflake.py:135} INFO - Executing COPY command...
[2022-08-11 05:57:46,881] {base.py:68} INFO - Using connection ID 'snowflake_conn' for task execution.
[2022-08-11 05:57:46,887] {connection.py:275} INFO - Snowflake Connector for Python Version: 2.7.11, Python Version: 3.7.13, Platform: Linux-5.10.104-linuxkit-aarch64-with-debian-11.3
[2022-08-11 05:57:46,889] {connection.py:927} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2022-08-11 05:57:46,892] {connection.py:944} INFO - Setting use_openssl_only mode to False
[2022-08-11 05:57:47,706] {cursor.py:715} INFO - query: [ALTER SESSION SET autocommit=True]
[2022-08-11 05:57:47,968] {cursor.py:739} INFO - query execution done
[2022-08-11 05:57:47,970] {snowflake.py:328} INFO - Running statement: COPY INTO public.food_consumption_estimates
FROM @s3_stage/
files=('food-consumption-estimates-staged.csv')
file_format=csv_format, parameters: None
[2022-08-11 05:57:47,971] {cursor.py:715} INFO - query: [COPY INTO public.food_consumption_estimates FROM @s3_stage/ files=('food-consump...]
[2022-08-11 05:57:48,155] {cursor.py:739} INFO - query execution done
[2022-08-11 05:57:48,159] {connection.py:557} INFO - closed
[2022-08-11 05:57:48,304] {connection.py:560} INFO - No async queries seem to be running, deleting session
[2022-08-11 05:57:48,459] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/transfers/s3_to_snowflake.py", line 136, in execute
    snowflake_hook.run(copy_query, self.autocommit)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 332, in run
    cur.execute(sql_statement)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/cursor.py", line 804, in execute
    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 280, in errorhandler_wrapper
    error_value,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 331, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 217, in default_errorhandler
    cursor=cursor,
snowflake.connector.errors.ProgrammingError: 002003 (02000): 01a635c5-0004-286b-001e-0007000312da: SQL compilation error:
Stage 'US_FOOD_NUTRITION.PUBLIC.S3_STAGE' does not exist or not authorized.
[2022-08-11 05:57:48,471] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_DAG_V2, task_id=load_into_food_consumption_estimates_table, execution_date=20220811T055731, start_date=20220811T055746, end_date=20220811T055748
[2022-08-11 05:57:48,480] {standard_task_runner.py:97} ERROR - Failed to execute job 224 for task load_into_food_consumption_estimates_table (002003 (02000): 01a635c5-0004-286b-001e-0007000312da: SQL compilation error:
Stage 'US_FOOD_NUTRITION.PUBLIC.S3_STAGE' does not exist or not authorized.; 18179)
[2022-08-11 05:57:48,504] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-08-11 05:57:48,534] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-08-11 19:17:02,954] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: ETL_DAG_V2.Load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [queued]>
[2022-08-11 19:17:02,971] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: ETL_DAG_V2.Load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [queued]>
[2022-08-11 19:17:02,973] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-08-11 19:17:02,973] {taskinstance.py:1377} INFO - Starting attempt 1 of 6
[2022-08-11 19:17:02,974] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-08-11 19:17:02,986] {taskinstance.py:1397} INFO - Executing <Task(S3ToSnowflakeOperator): Load_into_food_consumption_estimates_table> on 2022-08-11 05:57:31.582348+00:00
[2022-08-11 19:17:02,994] {standard_task_runner.py:52} INFO - Started process 1417 to run task
[2022-08-11 19:17:03,002] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'ETL_DAG_V2', 'Load_into_food_consumption_estimates_table', 'manual__2022-08-11T05:57:31.582348+00:00', '--job-id', '443', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpkmkz0jk_', '--error-file', '/tmp/tmp6j1qluja']
[2022-08-11 19:17:03,005] {standard_task_runner.py:80} INFO - Job 443: Subtask Load_into_food_consumption_estimates_table
[2022-08-11 19:17:03,139] {task_command.py:371} INFO - Running <TaskInstance: ETL_DAG_V2.Load_into_food_consumption_estimates_table manual__2022-08-11T05:57:31.582348+00:00 [running]> on host ce1be8c59d56
[2022-08-11 19:17:03,367] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=narapady
AIRFLOW_CTX_DAG_ID=ETL_DAG_V2
AIRFLOW_CTX_TASK_ID=Load_into_food_consumption_estimates_table
AIRFLOW_CTX_EXECUTION_DATE=2022-08-11T05:57:31.582348+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-08-11T05:57:31.582348+00:00
[2022-08-11 19:17:03,371] {s3_to_snowflake.py:135} INFO - Executing COPY command...
[2022-08-11 19:17:03,402] {base.py:68} INFO - Using connection ID 'snowflake_conn' for task execution.
[2022-08-11 19:17:03,407] {connection.py:275} INFO - Snowflake Connector for Python Version: 2.7.11, Python Version: 3.7.13, Platform: Linux-5.10.104-linuxkit-aarch64-with-debian-11.3
[2022-08-11 19:17:03,409] {connection.py:927} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2022-08-11 19:17:03,411] {connection.py:944} INFO - Setting use_openssl_only mode to False
[2022-08-11 19:17:04,417] {cursor.py:715} INFO - query: [ALTER SESSION SET autocommit=True]
[2022-08-11 19:17:04,596] {cursor.py:739} INFO - query execution done
[2022-08-11 19:17:04,598] {snowflake.py:328} INFO - Running statement: COPY INTO public.food_consumption_estimates
FROM @s3_stage/
files=('food-consumption-estimates-staged.csv')
file_format=csv_format, parameters: None
[2022-08-11 19:17:04,599] {cursor.py:715} INFO - query: [COPY INTO public.food_consumption_estimates FROM @s3_stage/ files=('food-consump...]
[2022-08-11 19:17:04,964] {cursor.py:739} INFO - query execution done
[2022-08-11 19:17:04,967] {snowflake.py:338} INFO - Statement execution info - {'file': 's3://s3-bucket-staged/food-consumption-estimates-staged.csv', 'status': 'LOAD_SKIPPED', 'rows_parsed': 0, 'rows_loaded': 0, 'error_limit': None, 'errors_seen': 1, 'first_error': 'File was loaded before.', 'first_error_line': None, 'first_error_character': None, 'first_error_column_name': None}
[2022-08-11 19:17:04,971] {snowflake.py:342} INFO - Rows affected: 1
[2022-08-11 19:17:04,972] {snowflake.py:343} INFO - Snowflake query id: 01a638e5-0004-2875-001e-0007000353e2
[2022-08-11 19:17:04,973] {connection.py:557} INFO - closed
[2022-08-11 19:17:05,217] {connection.py:560} INFO - No async queries seem to be running, deleting session
[2022-08-11 19:17:05,446] {s3_to_snowflake.py:137} INFO - COPY command completed
[2022-08-11 19:17:05,465] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=ETL_DAG_V2, task_id=Load_into_food_consumption_estimates_table, execution_date=20220811T055731, start_date=20220811T191702, end_date=20220811T191705
[2022-08-11 19:17:05,523] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-08-11 19:17:05,572] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
